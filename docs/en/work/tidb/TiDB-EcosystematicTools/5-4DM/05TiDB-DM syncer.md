---
title: DM syncer
description: DM syncer
---

# DM syncer

## What's syncer

1. Syncer can sync your MySQL data to another MySQL database.

    ![DM_Worker_Components](../../../../../images/tidb/05TiDB-EcosystematicTools/5-4DM/03-DM_Worker_Components.jpeg)

## Pessimist

1. I've talked a little bit what the worker does in the pessmistic DDL machinism, PTAL at [the pessmist content](./03TiDB-DM%20Master.md) first in DM Master.

2. From this `DM Syncer Main Steps` part of DM worker, we can see the syncer inside of DM Worker just put his DDL info and watch operation generated by DM Master, then put the query job to the `DDL job queue` if it's the owner of DDL Lock. It's pretty simple to do the work DM worker should do.

## Optimist

1. Also PLZ take a look on the one I've written [here](../5-4DM/03TiDB-DM%20Master.md), all because you may be confused without the pre-reading. Mostly there're relevent.

2. The pic on the top of this page, the only different processing step is just `switch op.ConflictStage` after `GetOperation`, though. They put totaly different info to etcd between pessmist and and optimist. I mean, after puting info to DM Master in optimist, DM Worker get and justify if there's conflict and to deal with next steps in a different way. At [here](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/ddl.go#L883) after adjusting conflict, what DM Worker actually get is gotten from DM Master.

## streamController

1. StreamerController including some items like `retryStrategy`, `syncCfg`, `fromDB` and so on in [it's struct](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/binlogstream/streamer_controller.go#L131), controls the streamer for read binlog, include reset streamer to a binlog position or GTID, read next binlog event, transfer from local streamer to remote streamer.

2. From the interface of [`GenerateStreamFrom`](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/binlogstream/streamer_controller.go#L54-L83) and [func GetEvent](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/binlogstream/streamer_controller.go#L337), can see it encapsulates status info of getting MySQL Binlog to the next one Event continuously.

## RunningLoop

1. Actually, `RunnLoop` is a name I created and it should be `func (s *Syncer) Run`. The reason I changed the name is make it more meaningful, because the key logic refer to [this for-loop](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1968-L1969) I've refered to inside the 694-rows-function.

2. As the pic showed, streamController will continuously send MySQL Binlog and the running dealing with everyone by `handleRotateEvent`,`HandleQueryEvent` and so on. I think that's the most important.

3. Inside [func handleRowsEvent](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L2535), there're too many event defined by MySQL and DM just needs to transform part of them, like `UPDATE_ROWS_EVENT`,`DELETE_ROWS_EVENT`,`WRITE_ROWS_EVENT` and so on. Syncer could be able to do that by [the package mysql-go](https://github.com/go-mysql-org/go-mysql) and some event defining info you can get at [the page_protocol_replication_binlog_event](https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_replication_binlog_event.html).

## syncDML

1. What this [func](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1534) does is just gets dmlJob continuously from dmlJobCh, and outputs it to compactor or causality. It varies according to if you configed `compact` as true, which's a tuning to accelerate the replicating speed. The [compact](https://docs.pingcap.com/zh/tidb/stable/dm-dml-replication-logic#compactor) is a new feature, which's used to compact some DMLs into only one DML, like : `INSERT + UPDATE => INSERT`, `INSERT + DELETE => DELETE`, etc. As we all know, some queries like `insert into XXX values (),() ...;` is pretty fast than the format like `insert into XXX values(); insert into XXX values(); ...` .

2. Meanwhile, the `Causality` couldn't be ignored, because the functionality is used to detect DML conflict instead of just replicating DML orderly。More info about the parallel algorithm of Causality at [this blog](https://cn.pingcap.com/blog/tidb-binlog-source-code-reading-8#%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8CDML), in short, every event'll be dispatched to different group of `Causality Tree` by PK or UK, if the key of later event has a conflict with the former. Such as the conflict could be `Event-1 : INSERT INTO table_name (pk, uk) VALUES (1, 2);` and `Event-4 : DELETE FROM table_name WHERE pk = 1;` must be executed in time order, vice versa, there'll be a conflict. DM, in that way, could able to sync DML parallely between different groups of `Causality Tree`. So, if Causality's detected any conflit, there must fistly sync queries already existed in the group to downstream(TiDB Cluster).

## syncDDL

1. IMHO, It's just a dispath component func which's aiming for DDL shard logic and record metrics. if be configured as `ShardPessimistic`,

2. Inside the [func syncDDL](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1356). It'll check config mode of [shardMode](https://docs.pingcap.com/tidb/stable/dm-shard-merge#configure-parameters) to do total different behaviours. For pessimist，PLZ look [the pessimist component in syncer](#pessimist) and [the pessimist component in DM Masrer](./03TiDB-DM%20Master.md). While, for optimist, PLZ look [the optimist component in DM Masrer](./03TiDB-DM%20Master.md) and [the optimist component in syncer](#optimist). I think it's the most complex and important logic in the entire DM.

## DDLWorker

1. Well, It's just a component to do work about DDL, also there're some key functions such as `HandleQueryEvent`, `skipQueryEvent`, `processOneDDL` and `handleDDL` of normal DDL, Optimist DDL and Pessimist DDL.

2. While, `HandleQueryEvent` connects the functions I metioned above. It'll record metrics and be responsible for split the type of DDL compacted multi DDLs in one SQL into some single ddls. because , until now, TiDB doesn't support the [one](https://docs.pingcap.com/tidb/stable/dm-faq#if-a-statement-executed-upstream-contains-multiple-ddl-operations-does-dm-support-such-migration).

## DMLWorker

1. Also the main functions are `dmlWorkerWrap`, `sendJobToAllDmlQueue`, `executeJobs`, `genSQLs`. DM has one config parameter named [worker-count](https://docs.pingcap.com/zh/tidb/stable/dm-tune-configuration#worker-count) accelerated DML replication speed. At `dmlWorkerWrap`, the step creates the number of the worker-count as concurrency to sync DMLs. DMLs were splited by [hashing every DML Key](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/dml_worker.go#L138) inside [func run](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/dml_worker.go#L101). By that, every queue executes in a batch way with parameter of [DM batch](https://docs.pingcap.com/tidb/stable/dm-tune-configuration#batch). In short, DMLWorker transforms job into SQL and flush them into downstream.

2. More details you can find at [here](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/dml_worker.go#L33).

## checkpointWorker

1. Well, checkPoint represents checkpoints status for syncer including global binlog's checkpoint and every table's checkpoint, when save checkpoint, we must differ saving in memory from saving (flushing) to DB (or file) permanently. for sharding merging, we must save checkpoint in memory to support skip when re-syncing for the special streamer, but before all DDLs for a sharding group to be synced and executed, we should not save checkpoint permanently. Because, when restarting to continue the sync, all sharding DDLs must try-sync again. Here's the key [interface](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/checkpoint.go#L227) for you to know how it does work.

2. In Syncer, [flushCheckPoints](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1187) and [flushCheckPointsAsync](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1223) call `checkpointFlushWorker.Add(task)` to push checkpoint task into checkpointWorker. There's also a loop [func Run](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/checkpoint_flush_worker.go#L91) to dispatch into sync/async logic and flush global checkpint using FlushPointsExcept into downstream checkpoint-table permantly. Untill now, async flush checkpoint hasn't been released, more info at the [RFC](https://github.com/pingcap/tiflow/blob/master/dm/docs/RFCS/20211012_async_checkpoint_flush.md).

3. There're four scenarios needing to flush checkpoint. A half of them are included in [func handleJob](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1048), and the others are in [func Syncer.Run](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1654).  
    a. **DDL executed** : when every DDL exists in DM, the flushing action'll be triggered at [func handleJob](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1101-L1142).  
    b. **pausing / stopping the sync (driven by `s.flushJobs`)** : flush all jobs before exit, at [func s.flushJobs()](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1870). Or there's any error solving using [skip-error methord](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L2174-L2203).  
    c. **IsFreshTask return true** : if streamController successfully started or the Mode configed as `ALL`, the checkpoint loaded from the dump metafile should be flushed.  
    d. **Heartbeat event received** : At [func flushIfOutdated](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L1030), when every event from MySQL Binlog is dispatched into DML Job queues, [here](https://github.com/pingcap/tiflow/blob/c65e2b72198de10319008b31dcf13d51509ccfde/dm/syncer/syncer.go#L2614).  

4. In short, checkpoint is an item to make sure the final consistency in data replication. What I mean is the one let DM could be able to continuous replicate after recovering from error, though, if there's any error.

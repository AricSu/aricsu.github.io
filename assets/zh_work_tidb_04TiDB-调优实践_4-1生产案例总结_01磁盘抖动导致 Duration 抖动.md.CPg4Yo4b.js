import{_ as i,c as t,o as a,a4 as e}from"./chunks/framework.svyMRVY3.js";const l="/assets/0.C44Jlc6L.png",o="/assets/11.5d5-3i7y.png",s="/assets/2.C8lC4AY_.png",n="/assets/3.yJsh3hyd.png",p="/assets/4.DZDR1q_Q.png",c="/assets/5.DoU6x7Gd.png",u="/assets/ComponentsOverview.o0YSOmuO.png",d="/assets/1.D7blhjLB.png",h="/assets/6.D4QIngmF.png",m="/assets/7.BqemYvJ_.png",g="/assets/18.Z1HFgjYJ.png",b="/assets/8.BxuSXI7F.png",P="/assets/9.BzASZ4-u.png",D="/assets/10.3W5Mnper.png",S="/assets/19.n27wH-e3.png",_="/assets/12.CPi9a47n.png",f="/assets/13.DpGqERAG.png",q="/assets/17.Cl4DNWVp.png",k="/assets/16.UOhGsKBA.png",L="/assets/14.CqGndeNc.png",T="/assets/15.Br8oxg9y.png",w=JSON.parse('{"title":"SQL Duration 抖动现象问题排查","description":"","frontmatter":{},"headers":[],"relativePath":"zh/work/tidb/04TiDB-调优实践/4-1生产案例总结/01磁盘抖动导致 Duration 抖动.md","filePath":"zh/work/tidb/04TiDB-调优实践/4-1生产案例总结/01磁盘抖动导致 Duration 抖动.md"}'),Q={name:"zh/work/tidb/04TiDB-调优实践/4-1生产案例总结/01磁盘抖动导致 Duration 抖动.md"};function B(I,r,y,C,E,x){return a(),t("div",null,r[0]||(r[0]=[e('<h1 id="sql-duration-抖动现象问题排查" tabindex="-1">SQL Duration 抖动现象问题排查 <a class="header-anchor" href="#sql-duration-抖动现象问题排查" aria-label="Permalink to &quot;SQL Duration 抖动现象问题排查&quot;">​</a></h1><p>时间：2021-02-02</p><h2 id="问题现象" tabindex="-1">问题现象 <a class="header-anchor" href="#问题现象" aria-label="Permalink to &quot;问题现象&quot;">​</a></h2><p>        在客户 TiDB 生产库进行巡检，发现 SQL Duration 存在突然小幅度升高后又回落的现象，需分析导致此现象的具体原因，以规避潜在风险。</p><ul><li>Metrics 中反映的问题现象<br><img src="'+l+'" alt="0"></li></ul><h2 id="排查过程" tabindex="-1">排查过程 <a class="header-anchor" href="#排查过程" aria-label="Permalink to &quot;排查过程&quot;">​</a></h2><h4 id="核心指标" tabindex="-1">核心指标 <a class="header-anchor" href="#核心指标" aria-label="Permalink to &quot;核心指标&quot;">​</a></h4><p>        本次仅介绍在排查问题过程中涉及到的指标</p><ul><li>PingLatency</li><li>QPS、Statement OPS</li><li>Parse Duration、Compile Duration、Execution Duration、Distsql Duration、Coprocessor Seconds</li><li>KV Request OPS、KV Request Duration 99 by store、KV Request Duration 99 by type</li><li>gRPC poll CPU、Scheduler worker CPU、Raft store CPUAsync aapply CPU</li><li>Scheduler writing bytes、Scheduler pending commands</li><li>Propose wait duration per server、Apply wait duration per server</li><li>Append log duration per server、Apply log per server、Commit log duration per server</li><li>Disk Lantency、Disk Load、Disk IOps</li></ul><h2 id="排查思路" tabindex="-1">排查思路 <a class="header-anchor" href="#排查思路" aria-label="Permalink to &quot;排查思路&quot;">​</a></h2><p>        首先，对可能引起 SQL Duration 抖动原因进行分类，追一排查否定不可能因素，直至定位根本原因；</p><ul><li>案例排查思路分类 <ol><li>网络延迟抖动性升高，导致 Duration 上升；</li><li>慢 SQL 导致的 Duration 上升；</li><li>集群组件性能问题导致的 Duration 上升；</li></ol></li></ul><h3 id="网络延迟抖动性方向排查" tabindex="-1">网络延迟抖动性方向排查 <a class="header-anchor" href="#网络延迟抖动性方向排查" aria-label="Permalink to &quot;网络延迟抖动性方向排查&quot;">​</a></h3><ul><li><p>排查思路<br>         网络延迟可能导致 RPC 消息传输慢，进而导致 SQL CMD 执行出现 Duration 的现象。</p><ul><li>PingLatency：指标记录网络延迟情况，问题时段并未出现网络异常现象；</li></ul></li><li><p>排查结果<br>         排除网络原因导致的 SQL Duration 升高的原因；</p></li><li><p>案例 Top SQL 截图<br><img src="'+o+'" alt="11"></p></li></ul><h3 id="top-sql方向排查" tabindex="-1">TOP_SQL方向排查 <a class="header-anchor" href="#top-sql方向排查" aria-label="Permalink to &quot;TOP_SQL方向排查&quot;">​</a></h3><ul><li><p>排查思路</p><ol><li>通过 slow_query 系统信息表相应字段分组排序，查出巡检时间内所需的 Top SQL 信息；</li><li>如果断定 SQL 是引起 Duration 升高的主要原因，可通过 Slow Query File 进一步分析； <ul><li>查询 SQL 慢的阶段，如：Parse_time、Compile_time 等等，详细信息参考-<a href="https://docs.pingcap.com/zh/tidb/stable/identify-slow-queries#%E5%AD%97%E6%AE%B5%E5%90%AB%E4%B9%89%E8%AF%B4%E6%98%8E" target="_blank" rel="noreferrer">慢查询日志字段含义说明</a></li><li>查询 SQL 历史执行计划，如：select tidb_decode_plan(...)，优化对性能瓶颈起决定性作用的执行计划，详细信息参考-<a href="https://docs.pingcap.com/zh/tidb/stable/identify-slow-queries#%E7%9B%B8%E5%85%B3%E7%B3%BB%E7%BB%9F%E5%8F%98%E9%87%8F" target="_blank" rel="noreferrer">查看 Plan</a></li><li>定位原因后通过 Hint 或 Index 优化慢 SQL</li></ul></li></ol></li><li><p>排查结果<br>         案例中 TiDB 集群共有4台 TiDB 实例，分别是 IP88、IP89、IP91、IP93，在四个台 TiDB 实例上分别取问题时间段半小时的 Slow Query 情况，发现并没有慢 SQL 执行次数多到足够影响整个集群 99% 分位数的 Duration 升高。<br>         因此，慢 SQL 导致 Duration 的方向锁定问题原因的思路被排除。</p></li><li><p>案例 Top SQL 截图</p><ul><li><p>IP88<br><img src="'+s+'" alt="2"></p></li><li><p>IP89<br><img src="'+n+'" alt="3"></p></li><li><p>IP91<br><img src="'+p+'" alt="4"></p></li><li><p>IP93<br><img src="'+c+'" alt="5"></p></li></ul></li></ul><h3 id="集群组件性能问题方向排查" tabindex="-1">集群组件性能问题方向排查 <a class="header-anchor" href="#集群组件性能问题方向排查" aria-label="Permalink to &quot;集群组件性能问题方向排查&quot;">​</a></h3><ul><li><p>组件关系图，参考<a href="https://download.pingcap.com/images/docs-cn/performance-map.png" target="_blank" rel="noreferrer">官方问文档 Performance-map</a> 总结 <img src="'+u+'" alt="组件关系图"></p></li><li><p>排查思路<br>         导致 SQL Duration 的原因也可能是集群组件出现问题导致的，排查思路为依据 TiDB 各组件间关系、SQL 执行流程等体系知识，把握 Promethus 核心监控指标，自定向下逐层深挖各组件影响性能最大的因素。</p></li></ul><h3 id="query-summary" tabindex="-1">Query-Summary <a class="header-anchor" href="#query-summary" aria-label="Permalink to &quot;Query-Summary&quot;">​</a></h3><ul><li><p>排查思路：<br>         SELECT、INSERT、UPDATE、DELETE 中任何类型 SQL 的任何一种都有可能导致 Duration 升高，应该通过 Statement OPS 找出其中占比重比较大的 SQL 操作。因为 SQL 占比重较小的 SQL 即使很慢，也很小概率会出现在 99% 分位数的视图中，所以应先对 SQL 操作分类排查;</p><ul><li>QPS：指标折线图显示问题时段 QPS 与正常时段无差异，说明 Client 请求没有增多；</li><li>Statement OPS：指标显示 select、Insert 操作居多，update、delete 极少，因为已经排除慢 SQL 问题，所以更倾向于怀疑 Insert CMD 导致 Duration 升高；</li></ul></li><li><p>排查结果<br>         SQL Duration 抖动的原因，从 Metrics 中推断更可能是 INSERT 导致的且 Client 端请求没有增加，佐证了可能是集群中组件出现性能瓶颈导致 Duration 抖动；</p></li><li><p>案例 Metrics<br><img src="'+d+'" alt="1"></p></li></ul><h2 id="排查细节" tabindex="-1">排查细节 <a class="header-anchor" href="#排查细节" aria-label="Permalink to &quot;排查细节&quot;">​</a></h2><p>        通过在三个方向的探索发现，基本锁定导致 Duration 抖动的原因极有可能是因为集群组件性能问题导致的 Duration 上升；下面，通过 Metrics 逐层排查原因。</p><h3 id="tidb部分组件排查" tabindex="-1">TiDB部分组件排查 <a class="header-anchor" href="#tidb部分组件排查" aria-label="Permalink to &quot;TiDB部分组件排查&quot;">​</a></h3><h4 id="tidb-executer" tabindex="-1">TiDB-Executer <a class="header-anchor" href="#tidb-executer" aria-label="Permalink to &quot;TiDB-Executer&quot;">​</a></h4><ul><li><p>排查思路<br>         Executer 主要职责为 AST --&gt; Logical Plan --&gt; Physcial Plan 执行链路上的转换与优化操作；</p><ul><li>Parse Duration 与正常阶段相比无明显异常；</li><li>Compile Duration 与正常阶段相比无明显异常；</li></ul></li><li><p>排查结果<br>         基本排除 TiDB Executer 层组件瓶颈问题导致的 SQL Duration 升高；但 Execution Duration 有明显抖动，需深挖原因；</p></li><li><p>案例 Metrics<br><img src="'+h+'" alt="6"></p></li></ul><h4 id="tidb-distsql" tabindex="-1">TiDB-DistSQL <a class="header-anchor" href="#tidb-distsql" aria-label="Permalink to &quot;TiDB-DistSQL&quot;">​</a></h4><ul><li><p>排查思路<br>         DistSQL 并行处理各 SQL 下推到各 TiKV 节点的 Coprocessor 处理操作，IP91 虽然较高，但峰值 23ms 的 Duration 并不能说明问题；</p><ul><li><strong>DistSQL Duration 有小幅度升高</strong>，说明此时可能存在汇总查询类的慢 SQL，在 TOP SQL 方向排查过程中也可以看到 IP91 节点存在一条执行两次的平均执行时间 SQL 达 26s 的慢SQL，但不足以影响整个集群 SQL Duration；</li><li><strong>Coprocessor Seconds 0.999 分位数</strong>，四台 TiDB 实例均幅度不等升高，并不能说明问题；</li></ul></li><li><p>排查结果<br>         TiDB 在 DistSQL 处理阶段不存在性能问题；综上所述，基本排除 TiDB 层存在性能问题的情况；</p></li><li><p>案例 Metrics<br><img src="'+m+'" alt="7"><br><img src="'+g+'" alt="18"></p></li></ul><h4 id="tidb-kv" tabindex="-1">TiDB-KV <a class="header-anchor" href="#tidb-kv" aria-label="Permalink to &quot;TiDB-KV&quot;">​</a></h4><ul><li><p>排查思路：<br>         KV 线程池处理阶段处于 Transaction 处理阶段和 TiKV Client 阶段处理之间，判断 TiDB 在开启事务后处理 KV 数据是否存在性能瓶颈。</p><ul><li>KV Request OPS：指标折线显示其他节点 OPS 阶段曲线在问题时间不存在明显特别，仅有 IP91:10080-Cop 在问题阶段出现 OPS 升高的情况，但从慢查询结果来看，并无与 SELECT 有关慢查询，<strong>可能单纯是发往 IP91:10080 的 Cop 操作过多</strong>，推测有可能是 IP91 上两条平均执行 26s 慢 SQL 导致的，<strong>不应该是主要关注点</strong>；</li><li>KV Request Duration 99 by store：TiDB 中的 KV 阶段发向 store4 的请求存在执行缓慢现象，<strong>折线反映飞非常明显</strong>；</li><li>KV Request Duration 99 by type：很有可能与写入有关，发生在 Prewrite 阶段，<strong>折线反映飞非常明显</strong>；</li></ul></li><li><p>排查结果<br>         基本判断在 Prewrite 阶段出现了问题，接下来判断是哪个或那几个节点在 Prewrite 阶段出现了问题；</p></li><li><p>案例 Metrics<br><img src="'+b+'" alt="8"><br><img src="'+P+'" alt="9"></p></li></ul><h3 id="tikv部分组件排查" tabindex="-1">TiKV部分组件排查 <a class="header-anchor" href="#tikv部分组件排查" aria-label="Permalink to &quot;TiKV部分组件排查&quot;">​</a></h3><h4 id="tikv-grpc" tabindex="-1">TiKV-gRPC <a class="header-anchor" href="#tikv-grpc" aria-label="Permalink to &quot;TiKV-gRPC&quot;">​</a></h4><ul><li><p>排查思路<br>         gRPC 阶段属于 TiKV 端，用以接收来自 TiDB 端 TiKV Client 组件发送的请求。</p><ul><li>99% 分位数显示 kv_prewrite-IP92:270172 在处理 kv-write Duration 峰值在 7s 左右，明显区别于其他节点<strong>折线显示非常明显</strong>；</li><li>图二所示，基本排除因为分配 CPU 核数不足导致的各阶段 ThreadPool 瓶颈问题，<strong>gRPC poll CPU、Scheduler worker CPU、Raft store CPU、Async apply CPU 均有很大可利用空间</strong>；</li></ul></li><li><p>排查结果<br>         基本判断 IP92:270172 出现问题。接下来深挖问题原因，大概率与 Prewrite 阶段 I/O 问题有关。</p></li><li><p>案例 Metrics<br><img src="'+D+'" alt="10"><br><img src="'+S+'" alt="19"></p></li></ul><h4 id="tikv-scheduler" tabindex="-1">TiKV-Scheduler <a class="header-anchor" href="#tikv-scheduler" aria-label="Permalink to &quot;TiKV-Scheduler&quot;">​</a></h4><ul><li><p>排查思路<br>         Scheduler 阶段负责处理发往 TiKV 阶段的写请求，依据此阶段 Metrics 进一步佐证在 gRPC 阶段得出的 kv_prewrite-IP92:270172 节点写请求出现性能问题的结论，并通过 IP:Port 所启动的进程查出具体的 Store；</p><ul><li>Scheduler writing bytes：指标显示 IP92、IP100 在问题时段等待写入的数据量存在明显增长，极有可能存在数据积压，未能及时将数据写入到 RaftStore 中的情况；</li><li>Scheduler pending commands：指标显示 IP92 在问题时间待处理的命令出现积压，<strong>进一步佐证 IP92 写入出现问题</strong>；</li></ul></li><li><p>排查结果<br>         基本确定 IP92:270172 对应的 Store 出现了性能问题，需查看问题时段 Disk-Performance 最终发掘问题根本原因；<br>         通过第二张图片，锁定对应 Store 为 /dev/sdc 磁盘；</p></li><li><p>案例 Metrics<br><img src="'+_+'" alt="12"><br><img src="'+f+'" alt="13"></p></li></ul><h4 id="tikv-raftio" tabindex="-1">TiKV-RaftIO <a class="header-anchor" href="#tikv-raftio" aria-label="Permalink to &quot;TiKV-RaftIO&quot;">​</a></h4><ul><li><p>排查思路<br>         RaftIO 监控 RaftStore 阶段，RaftStore 用于存储实现 Raft 协议阶段所需要的数据。各指标发生阶段、参数理想值详细参操-<a href="https://download.pingcap.com/images/docs-cn/performance-map.png" target="_blank" rel="noreferrer">官方问文档 Performance-map</a></p><ul><li>Propose wait duration per server：指标显示 IP92:270172 峰值达到 3.936s，<strong>说明在处理 raft Log 时比较慢</strong>；</li><li>Apply wait duration per server：指标显示 IP92:270172 峰值达到 213ms，performance-map 推荐 99% 分位数值小于 50ms，<strong>说明在 raft 数据落盘时比较慢</strong>；</li></ul></li><li><p>排查结果<br>         推测可能大量 RaftStore 数据存在于 RaftStore 线程池的 channel 中，未能及时被相应处理线程消费写入 rocksDB raft 中，<strong>此时因为 CPU 未被打满，所以更加怀疑磁盘性能问题</strong>；</p></li><li><p>案例 Metrics<br><img src="'+q+'" alt="17"></p></li></ul><h4 id="tikv-apply" tabindex="-1">TiKV-Apply <a class="header-anchor" href="#tikv-apply" aria-label="Permalink to &quot;TiKV-Apply&quot;">​</a></h4><ul><li><p>排查思路</p><ul><li>Append log duration per server：指标显示 IP92:270172 峰值达到 488ms，<strong>说明在 raft Log 时出现问题</strong>；</li><li>Apply log per server：指标显示 IP92:270172 峰值达到 471ms，performance-map 推荐 99% 分位数值小于 100ms，<strong>说明在 raft 数据落盘时出现问题</strong>；</li><li>Commit log duration per server：指标显示 IP92:270172 峰值达到 1.95s，<strong>说明记录 Commit Log 时出现问题</strong>；</li></ul></li><li><p>排查结果<br>         IP92:270172 对应 Store 上进行的 Append、Apply、Commit 操作均出现延迟现象，需要查看 apply 阶段 IP92:270172 Store 磁盘的写入情况，进一步佐证了磁盘性能出现问题的猜想；</p></li><li><p>案例 Metrics<br><img src="'+k+'" alt="16"></p></li></ul><h4 id="disk-performance" tabindex="-1">Disk-Performance <a class="header-anchor" href="#disk-performance" aria-label="Permalink to &quot;Disk-Performance&quot;">​</a></h4><ul><li><p>排查思路<br>         Disk-Performance Metrics 监控各 Store 的磁盘性能，包含 Lantency、IOps、BandWidth、Load 等信息；</p><ul><li>Disk Lantency：指标显示问题时段延迟高达 15ms，<strong>与正常时段比较属于高延时</strong>；</li><li>Disk Load：指标显示问题时段负载高达 8.77，<strong>与正常时段比较属于高负载状态</strong>；</li><li>Disk IOps：指标显示问题时段 IOps 在 107 左右，<strong>在正常时段比较属于极低状态</strong>；</li></ul></li><li><p>排查结果<br>         查看 Store 问题时段状态，发现磁盘延时、负载高出正常水平，但 IOps 却极低，说明问题时段出现了磁盘性能抖动。</p></li><li><p>案例 Metrics<br><img src="'+L+'" alt="14"><br><img src="'+T+'" alt="15"></p></li></ul><h2 id="问题解决" tabindex="-1">问题解决 <a class="header-anchor" href="#问题解决" aria-label="Permalink to &quot;问题解决&quot;">​</a></h2><p>        因为此次 SQL Duration 升高是 IP92 节点 Store 对应磁盘出现性能抖动导致的，无法弥补且不存在实质性风险，所以不用解决。</p><h2 id="归纳总结" tabindex="-1">归纳总结 <a class="header-anchor" href="#归纳总结" aria-label="Permalink to &quot;归纳总结&quot;">​</a></h2><p>        对于组件间的性能排查，熟知 <a href="https://download.pingcap.com/images/docs-cn/performance-map.png" target="_blank" rel="noreferrer">官方问文档 Performance-map</a> 各组件间关系，及对应 Metrics 的性能理想值非常关键。</p><h2 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h2><p><a href="https://download.pingcap.com/images/docs-cn/performance-map.png" target="_blank" rel="noreferrer">官方问文档 Performance-map</a></p><p><a href="https://docs.pingcap.com/zh/tidb/stable/identify-slow-queries#%E5%AD%97%E6%AE%B5%E5%90%AB%E4%B9%89%E8%AF%B4%E6%98%8E" target="_blank" rel="noreferrer">慢查询日志字段含义说明</a></p>',47)]))}const V=i(Q,[["render",B]]);export{w as __pageData,V as default};

# TiDB 监控实例分析


简介
TiDB 默认使用时序数据库 Prometheus 来监控集群状态并存储监控信息，通过监控可以直观的观察到集群各个模块运行的状态，更方便定位问题以及针对性的进行系统优化。监控系统架构如图：
     
TiDB 各服务（PD、TiKV、TiDB）运行过程中会以 Push 方式（默认 15s 的频率）将内部的监控信息推送给 PushGateWay
Prometheus 会定时从 PushGateWay 以 Pull 的方式拉取监控信并实例化存储到本地磁盘
Grafana 相当于一个 Client，从 Prometheus 读取监控数据并以 WEB 形式展示出来
AlertManager 是监控系统中的告警组件，可以以 Slack、邮件等形式发出告警

在这个文章中，我们列举一些具体案例结合监控来分析集群存在的问题，给用户提供一个处理问题的思路，方便用户更好的使用 TiDB。
注：以下监控都是选取采用比较常用的方式来参考分析，不排除还有其他办法，这里不一一列举。
一、重点监控内容
首先我们从整个集群入手列举需要关注的监控项。
1、PD
Leader、Region 调度，各 TiKV 节点空间使用情况
集群总空间使用情况
HotRegion 判断是否存在读写热点
2、TiDB
QPS、TPS
业务 SQL 延迟
事务冲突
连接数
向 TiKV 发送的请求信息和延迟
向 PD 发送的请求信息和延迟
GC
3、TiKV
TiKV 进程的内存使用情况
Leader、Region 分布状态
错误信息（server is busy、server report failures、leader drop、channel full 等）
各 TiKV 节点读写数据的量
coprocessor 判断是否扫表比较多
各 TiKV 节点不同线程使用 CPU 的情况
是否存在大 region
task 判断是否有过多堆积的消息
Grpc 的请求数量和延迟
4、Node_exporter
CPU
内存
网卡流量
TCP Retry 等
5、Disk-Performance
磁盘读写流量
磁盘 IO Util
磁盘读写延迟
6、Black_exporter
网络延迟
二、读写热点
1、热点的概念及产生原因
首先我们不得不提一下 TiDB 和 MySQL 的区别。和 MySQL 的单机存储、计算不同，对于分布式数据库 TiDB 来说，数据和计算都是分散到集群中的多个 TiKV 节点上。在 TiKV 中数据是按照 key range 来切分 region，并以 region 为单位来调度并且存储到不同 TiKV 节点上（可以参考 PingCAP 官网文章三篇文章了解 TiDB 技术内幕 - 说存储来理解）。
热点的表象是在集群中总会有某个或者某几个 TiKV 的资源使用明显比其他节点高，热点分为写热点和读热点，下面分别介绍产生写热点和读热点的原理和场景。
写热点。通过上面的解释我们已经清楚，数据是按 range 切分的，当数据都是连续写入时，会只在一个 TiKV 节点插入（当然也会通过 raft 同步到其他两个 follower 节点），这个时候压力就会落在少数几个节点上，产生写热点。主要有以下几种场景（主键和写热点的关系可以参考 PingCAP 官网文章三篇文章了解 TiDB 技术内幕 - 说计算来理解）：
表存在自增的 int 类型主键
表存在非自增 int 类型主键，但是业务上没有打散
表没有主键，默认时也是顺序写入的，可以通过 TiDB 的 SHARD_ROW_ID_BITS 参数来打散
读热点。同理，如果业务 SQL 查询的数据落在一个 TiKV 或者少数几个 TiKV 上，就会产生读热点，主要有以下两种场景：
小表频繁访问产生热点。这种情况是因为数据量太小了，在存储中可能只有 1 个 region 或者少数几个 region，不足以在 n 个 TiKV 节点之间调度均衡。
前提表存在自增的 int 类型主键，与业务 SQL 相关。例如按时间顺序写入，并且业务 SQL 都是按时间范围来查询，会存在读热点。
2、热点的影响
写热点会导致某个或者少数几个 TiKV 节点成为瓶颈，集群的写入能力不能完全发挥出来；比较严重的情况会由于单线程的 raftstore 打满影响 raft 调度，进而会导致整个集群抖动；存在写热点的集群一般也会伴随着读热点。
读热点会导致某个或者少数几个 TiKV 的资源使用增加，主要包括 coprocessor cpu 和磁盘 IO，当资源使用基本打满时，当前的 TiKV 会由于负载原因导致各种请求的响应都变慢，产生木桶效应影响整个集群。
3、结合监控分析写热点
raft store CPU

raft store 是用来处理 region 的调度、迁移、复制等工作的线程（目前是单线程的），当集群数据量太大或者写入太高的时候，会产生更多的调度、迁移、复制等操作，raft store CPU 的使用率会明显增长。通过观察监控，当发现有某个或者某几个 TiKV 明显比其他节点高时，基本可以判断这几个 TiKV 是存在写热点的。
GRPC message count

TiDB 集群内部是通过 GRPC 来通信的，可以通过观察不同 TiKV 接收到的 GRPC 消息的数量来判断是否有热点。通过上面两个监控可以看到不同 TiKV 写入（prewrite 和 commit）的量相差很多，判断出存在写热点。
此监控需要修改 Grafana：
Query 修改为：sum(rate(tikv_grpc_msg_duration_seconds_count{type!="kv_gc"}[1m])) by (type,job)
Legend format 修改为：{{job}} - {{type}}
4、结合监控分析读热点
Coprocessor CPU

Coprocessor 是 TiKV 查询相关的线程，当用户在 TiDB 上发起一个查询，TiKV 就会通过此线程来获取相应的数据。当某个或者某几个 TiKV 的 Coprocessor CPU 明显偏高，可以定位存在读热点的 TiKV 节点。
GRPC message count
参考写热点监控截图，关注 coprocessor、get、batch_get 监控项来定位读热点 TiKV 节点。
5、总结
通过以上方法，我们已经了解了什么场景会产生热点，也可以借助监控定位到哪些节点存在热点，更方便处理问题。但是，只是观察监控，不能明确定位到表，这里我们就需要通过分析日志来确定是哪些表存在热点，并结合业务来调整。
二、事务冲突
1、事务冲突原因和影响简述
简单来说，当两个事务同时对一行数据做操作时就会出现事务的冲突。在 TiDB 中，事务操作过程中会对相应的数据加锁，以免被其他事务修改，当一个事务涉及到的某行数据已经被加锁，这个事务会检查这行数据的锁是否已经过期、是否能被清理等，当判断这个锁不能做相关操作，此事务就会发生重试。当事务冲突严重时，会产生大量的重试，用户的直观感受就是响应变慢。
2、结合监控分析事务冲突
KV Backoff OPS

集群在请求超时、重试等的情况下会产生 backoff，backoff 又分很多中情况，其中 txnLock、txnLockFast 说明遇到了事务冲突，当监控的值比较高时，业务上会反映出有比较明显的延迟。
Lock Resolve OPS

not_expired 和 expired 监控项表示检查锁是否过期，resolve 表示清锁操作。当相关监控项的值比较高时，说明系统中频繁会有事务对应的数据被加锁，也就是有其他事务在对数据做操作（即存在事务冲突）。
3、总结
我们通过监控能够判断集群是否存在事务冲突，定位存在冲突的表需要分析集群日志来确定，在 TiDB 日志中能观察到 WriteConflict（事务冲突）和 Retry（重试的 SQL），这里不再赘述。
三、高写入瓶颈
1、高写入场景瓶颈
高写入场景常遇到的瓶颈有如下几个：
存在写热点
单线程的 raftstore 的 CPU 被打满
TiKV 节点磁盘 IO 使用率接近 100%
网络带宽被用满
TiKV 节点磁盘写入延迟高
2、结合监控分析写入瓶颈
写热点
写热点会造成木桶效应，一个或者几个节点的压力会影响整个集群的性能，具体内容在上文已经做了相关说明，可以参考分析。
raft store CPU
上文已经提到 raftstore 目前是单线程的，当这个线程的压力被打满时，会影响集群的数据调度、复制等操作，从而影响集群的性能。假设集群没有写热点，写入量很大的时候，也可能使集群多个 TiKV 的 raft store CPU 使用率在 80% 以上（一般在 80% 就会影响性能了），目前阶段可以通过调整参数来减小影响，或者通过扩容集群的 TiKV 实例数量来解决这个问题。
我们现在也正在优化多线程 raftstore，等完善之后就可以解决单线程的限制。
磁盘 IO utilization

通过监控可以看到各个 TiKV 节点对应磁盘的 IO 使用情况，当磁盘 IO 使用均接近满负载时，说明已经到了整个集群能承载的峰值，这时需要添加节点来支撑更高的性能要求。当然也有可能是热点或者磁盘本身性能问题导致的某个节点磁盘负载高，需要用户根据实际情况来分析。
网络带宽

由于 TiDB 本身是一个分布式系统，并且默认最低是三副本，不同组件之间就会涉及到很多的网络交互，当网络带宽率先成为瓶颈被打满时，集群性能也无法再提升，所以我们推荐在生产环境使用万兆网络，以免带宽成为整个集群的瓶颈。
磁盘写入延迟

磁盘写入延迟是经常会被忽略的一个方向，当集群没有其他异常（热点、磁盘 IO、网络带宽等）时，不妨查看一下磁盘的写入延迟，如果写入延迟比较高，很有可能是导致写入慢的原因。有相关经验的同学可能也遇到过磁盘 IO 使用并不高，而延迟明显异常的情况。
3、总结
目前版本的 raftstore 算是一个痛点，可能出现集群还有很多资源，但是 raftstore 线程被打满了，导致性能不能完全发挥出来，等多线程 raftstore 优化完成之后就能解决这个问题。至于其他类型的写入瓶颈，还是有很多针对性的办法来处理的。
四、慢查询
这个章节我们来分析一下用户经常纠结的慢查询（是否属于慢查询是根据具体业务来看的），慢查询和前几个章节的问题是有关联的，所以我们放到后面来讲，更方便理解。
1、慢查询分析思路
通过监控，慢查询大致可以从以下几个方向来分析：
监控定位哪个 TiDB 节点在什么时间出现了慢查询
监控定位是否存在事务冲突
监控定位是否存在读热点
查看 Coprocessor 监控定位是否扫表比较多
TiKV 集群是否处于繁忙状态
除此之外，还需要针对具体的 SQL 来看执行计划来分析，本文中我们先忽略。
2、结合监控分析慢查询
Duration 999 by Instance

通过监控，可以很简单的定位到哪个 TiDB 节点在什么时间出现了慢查询，然后我们再根据这个时间范围去查其他监控
如果能定位到具体的 SQL，建议先根据表结构和执行计划分析一下是否需要对 SQL 本身做优化，很多时候对 SQL 优化就已经能解决问题
参考上文查看集群是否存在事务冲突
参考上文查看集群是否存在读热点
Coprocessor 分析是否存在很多的扫表操作

当 tblscan 比较多时，说明有一些 SQL 会做扫表操作，一般来说扫表都是比较重的操作，SQL 本身会执行比较长时间，而且会占用比较多的资源，也有可能导致正常的 SQL 出现慢查询。
通过查看集群的 CPU、磁盘、网络等信息判断是否当时整个集群都处于繁忙状态





当 raft store CPU 打满时，可能会造成掉 leader 的现象，集群在这段时间就会出现抖动，导致慢查询；当 CPU、io utilization、Coprocessor CPU 等已经打满时，说明集群已经处于高负载的状态，需要分析是 SQL 的执行计划不对导致的还是确实已经没有优化空间了。
3、总结
慢查询涉及的内容比较多，需要结合其他处理方式一起分析。
